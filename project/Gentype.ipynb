{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLA-A allele clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gentype import EnsemblClient, DataManager, PiCollapsedNonparametricGibbsSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Database_Name = \"Gentype_DB.db\"\n",
    "client = EnsemblClient()\n",
    "data_manager = DataManager(client, Database_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize storable data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched = False\n",
    "inference_matrix = None\n",
    "models = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore stored data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -d fetched # if you want to refetch the data, uncomment this line\n",
    "#%store -d inference_matrix # if you want to recompute the inference_matrix, uncomment this line\n",
    "#%store -d models # if you want to recompute the models, uncomment this line\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Collecting and preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetches all HLA-A data obtained in 1000 Genomes project phase3 and stores them in a local sqlite server. This only needs to be done once but might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'fetched' (bool)\n"
     ]
    }
   ],
   "source": [
    "if not fetched:\n",
    "    data_manager.fetch_all(29941260, 29945884, \"6\", report_progress = True)\n",
    "fetched = True\n",
    "%store fetched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following generates an inference matrix from the data provided by the local database. The inference matrix can be constructed with respect to a population and a section specified by start and end. Be sure to fetch the according population before constructing the matrix. If _sum_allels = True_ is passed, the expression for an individual per allele will be summed (if expressed on both -> 2, on one -> 1, on neither -> 0). Otherwise each strand will be represented by a seperate row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Unknown variable 'inference_matrix,'\n"
     ]
    }
   ],
   "source": [
    "if inference_matrix is None:\n",
    "    inference_matrix, individual_map, variation_map = data_manager.generate_inference_matrix(start = 29941260, end = 29945884, population = \"ALL\")\n",
    "%store inference_matrix, individual_map, variation_map\n",
    "inference_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean number of variations per allele, for HLA-A as expected high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_matrix.sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train models with different initializations without seeding.\n",
    "\n",
    "Here we compute different models with varying dirichlet prior alpha. This prior is proportional to the probability of creating a new cluster, therefore inference with high alphas tends to begin with a higher number of clusters. This might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if models is None:\n",
    "    models = [PiCollapsedNonparametricGibbsSampler(42), PiCollapsedNonparametricGibbsSampler(42), PiCollapsedNonparametricGibbsSampler(42)]\n",
    "    for i in range(0, len(models)):\n",
    "        print(\"Model {}:\".format(i))\n",
    "        models[i].fit(inference_matrix, num_burn_in_steps = 100, delta = 1, alpha = 10**(i - 1))\n",
    "for i in range(0, len(models)):\n",
    "    print(\"Model {}:\".format(i))\n",
    "    plt.clf()\n",
    "    plt.plot(models[i].ll_list)\n",
    "    plt.show()\n",
    "\n",
    "%store models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May pre select models e.g. according to Aitikens Information Criterion (AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AIC(model):\n",
    "    size = np.prod(np.shape(model.theta))\n",
    "    loglikelihood =model.ll_list[-1]\n",
    "    return 2*size - 2*loglikelihood\n",
    "\n",
    "print(list(map(lambda x: AIC(x), models)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(inference_matrix)\n",
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 50% of the variation can be explained by the first two principle components !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = list(individual_map.keys())\n",
    "inds.extend(list(individual_map.keys()))\n",
    "\n",
    "populations = [\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\"]\n",
    "ind_map = data_manager.generate_individual_population_map()\n",
    "population_index = []\n",
    "for ind in inds:\n",
    "    for pop in ind_map[ind]:\n",
    "        if pop in populations:\n",
    "            population_index.append(populations.index(pop))\n",
    "pops = np.array(population_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Population distribution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "\n",
    "for i in range(len(pops)):\n",
    "    ax.plot(principalComponents[pops == i,0], principalComponents[pops == i,1], \"o\", alpha=0.5)\n",
    "ax.legend(populations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "\n",
    "for i in range(int(max(models[0].Z))):\n",
    "    ax.plot(principalComponents[models[0].Z == i,0], principalComponents[models[0].Z == i,1], \"o\", alpha=0.5)\n",
    "ax.legend(range(int(max(models[0].Z))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(zip([\"Cluster \" + str(i) for i in range(len(models[0].theta))], list(models[0].theta))))\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax = sns.heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following generates the distribution (as a dict) of amount of variations per strand in the specified region. I.e. {n : #strands with n variations}. And chooses one of the amounts according to the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = data_manager.get_variation_distribution(start = 17671934, end = 17681934, population = \"CHB\")\n",
    "choices = []\n",
    "probabilities = []\n",
    "for choice in distribution:\n",
    "    choices.append(choice)\n",
    "    probabilities.append(distribution[choice])\n",
    "choices = np.array(choices)\n",
    "probabilities = np.array(probabilities) / sum(probabilities)\n",
    "print(\"Choices: {}\".format(choices))\n",
    "print(\"Probabilities: {}\".format(probabilities))\n",
    "np.random.choice(choices, p=probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
